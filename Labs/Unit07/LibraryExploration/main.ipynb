{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch / TorchAudio\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Fade\n",
    "\n",
    "# MatPlotLib (Graphing)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mir_Eval for SDR (Signal-to-Distortion Ratio) calculations\n",
    "from IPython.display import Audio\n",
    "from mir_eval import separation\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
    "from torchaudio.utils import download_asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch + Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 44100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bundle = HDEMUCS_HIGH_MUSDB_PLUS # Pre-trained model\n",
    "\n",
    "model = bundle.get_model()\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cpu\") # (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Configure audio sample rate\n",
    "sample_rate = bundle.sample_rate\n",
    "\n",
    "print(f\"Sample rate: {sample_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split-Sources Function\n",
    "Note: This model is very memory-intensive, so a large portion of the program below is recommend by PyTorch to split the song and reduce memory load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sources(\n",
    "        model,\n",
    "        mix,\n",
    "        segment=10.0,\n",
    "        overlap=1,\n",
    "        device=None\n",
    "):\n",
    "    # Check that device is configured\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    # Get audio parameters\n",
    "    batch, channels, length = mix.shape\n",
    "\n",
    "    # Get chunk information\n",
    "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * sample_rate\n",
    "\n",
    "    # Configure fade (helps combine audio after splitting into chunks)\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\")\n",
    "\n",
    "    # Create a tensor filled with zeros which will be used for output\n",
    "    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        # Get current chunk\n",
    "        chunk = mix[:, :, start:end]\n",
    "\n",
    "        # Apply model\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(chunk)\n",
    "        \n",
    "        # Apply fade\n",
    "        out = fade(out)\n",
    "\n",
    "        # Append current chunk to final output\n",
    "        final[:, :, :, start:end] += out\n",
    "\n",
    "        # Configure fade in / out for next frame\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "            end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot spectogram function\n",
    "This will help us visualize the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(stft, title=\"Spectrogram\"):\n",
    "    # Calculate magnitude to find spectogram scale\n",
    "    magnitude = stft.abs()\n",
    "    # Update values using magnitutde\n",
    "    spectrogram = 20 * torch.log10(magnitude + 1e-8).numpy()\n",
    "    # Plot data\n",
    "    _, axis = plt.subplots(1, 1)\n",
    "    axis.imshow(spectrogram, cmap=\"viridis\", vmin=-60, vmax=0, origin=\"lower\", aspect=\"auto\")\n",
    "    axis.set_title(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load song\n",
    "waveform, sample_rate = torchaudio.load(\"./song.wav\")\n",
    "waveform = waveform.to(device)\n",
    "mixture = waveform\n",
    "\n",
    "# Configure parameters\n",
    "segment: int = 1000\n",
    "overlap = 0.1\n",
    "\n",
    "# Normalize Audio\n",
    "ref = waveform.mean(0)\n",
    "waveform = (waveform - ref.mean()) / ref.std()\n",
    "\n",
    "# Split sources\n",
    "sources = split_sources(\n",
    "    model,\n",
    "    waveform[None],\n",
    "    device=device,\n",
    "    segment=segment,\n",
    "    overlap=overlap,\n",
    ")[0]\n",
    "\n",
    "# Undo normalization\n",
    "sources = sources * ref.std() + ref.mean()\n",
    "\n",
    "# LOOK INTO BELOW (I don't know what it does)\n",
    "sources_list = model.sources\n",
    "sources = list(sources)\n",
    "\n",
    "audios = dict(zip(sources_list, sources))\n",
    "\n",
    "N_FFT = 4096\n",
    "N_HOP = 4\n",
    "stft = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=N_HOP,\n",
    "    power=None,\n",
    ")\n",
    "\n",
    "# Full Audio\n",
    "full = Audio(mixture, rate=sample_rate)\n",
    "with open(\"./full.wav\", \"wb\") as file:\n",
    "    file.write(full.data)\n",
    "\n",
    "# Drums Audio\n",
    "drums = Audio(audios[\"drums\"], rate=sample_rate)\n",
    "with open(\"./drums.wav\", \"wb\") as file:\n",
    "    file.write(drums.data)\n",
    "\n",
    "# Bass Audio\n",
    "bass = Audio(audios[\"bass\"], rate=sample_rate)\n",
    "with open(\"./bass.wav\", \"wb\") as file:\n",
    "    file.write(bass.data)\n",
    "\n",
    "# Vocals Audio\n",
    "vocals = Audio(audios[\"vocals\"], rate=sample_rate)\n",
    "with open(\"./vocals.wav\", \"wb\") as file:\n",
    "    file.write(vocals.data)\n",
    "\n",
    "# Other Audio\n",
    "other = Audio(audios[\"other\"], rate=sample_rate)\n",
    "with open(\"./other.wav\", \"wb\") as file:\n",
    "    file.write(other.data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
